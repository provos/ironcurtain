# Multi-Turn Scenario Generator with Prompt Caching

## Overview

The scenario generator currently makes a single LLM call per pipeline run. When the verifier discovers wrong expectations (deny vs escalate ambiguity, structural invariant conflicts), corrections are applied via `applyScenarioCorrections()` -- but on the next cache-miss run the generator repeats the same mistakes because it starts fresh. This design introduces an in-memory multi-turn conversation for the scenario generator so that corrections, discarded scenarios, and probe insights from the verify-repair loop can be fed back as follow-up messages, producing better scenarios on subsequent turns without re-sending (or invalidating) the large cacheable system prompt.

## Key Design Decisions

1. **New abstraction: `ScenarioGeneratorSession`** -- a stateful wrapper around the scenario generator that accumulates a message history across turns within a single pipeline run. The system prompt is set once at construction and never changes, enabling prompt caching. Each call to `generate()` appends a user message (with feedback) and the LLM's response to the history. This keeps the generator itself (`generateScenarios`) unchanged for backward compatibility.

2. **Feedback goes into user messages, not system prompts** -- corrections, discarded scenarios, and probe results are formatted as follow-up user messages in the conversation. This preserves the system prompt identity (critical for Anthropic prompt caching -- the system prompt must be bitwise identical across calls). The LLM sees: turn 1 = "generate scenarios", turn 2 = "here's what was wrong, generate replacements", etc.

3. **`generateObjectWithRepair` is not changed** -- it manages schema-repair within a single logical request. The multi-turn conversation is a higher-level concern: each turn is a separate `generateObjectWithRepair` call, with the session managing the message history across turns. This keeps the two layers orthogonal.

4. **The session is owned by `compile.ts`** -- the pipeline orchestrator creates the session before the first generation step and passes feedback to it during the repair loop. The session is not persisted to disk; it lives only for the duration of the pipeline run.

5. **The initial generation call remains cacheable** -- when the content-hash matches the existing artifact, the session is never created and no LLM call is made (existing cache behavior). The session only matters when the cache misses or when the repair loop needs to regenerate scenarios.

6. **Replacement, not accumulation** -- each subsequent turn asks the LLM to generate _replacement_ scenarios for the ones that were wrong, not an entirely new set. This keeps token usage bounded and avoids the combinatorial growth of scenarios across turns.

## Interface Definitions

### New type: `ScenarioFeedback`

```typescript
// src/pipeline/types.ts

/**
 * Feedback from the verify-repair loop that should be communicated
 * to the scenario generator on its next turn. Each field is optional;
 * only the feedback that applies to the current repair iteration is set.
 */
export interface ScenarioFeedback {
  /**
   * Scenarios whose expectations were corrected by the verifier judge.
   * The generator should avoid reproducing the original (wrong) expectations.
   */
  readonly corrections: ReadonlyArray<ScenarioCorrection>;

  /**
   * Scenarios discarded because they conflict with structural invariants.
   * The generator should not regenerate scenarios with the same tool+args+expectation.
   */
  readonly discardedScenarios: ReadonlyArray<DiscardedScenario>;

  /**
   * Probe scenarios generated by the verifier that revealed coverage gaps.
   * The generator can use these as hints for what areas need better coverage.
   */
  readonly probeScenarios: ReadonlyArray<TestScenario>;
}
```

### New class: `ScenarioGeneratorSession`

```typescript
// src/pipeline/scenario-generator.ts

/**
 * A stateful multi-turn wrapper around the scenario generator.
 *
 * Maintains a conversation history so that feedback from the verify-repair
 * loop (corrections, discarded scenarios, probes) can be communicated to
 * the LLM in follow-up turns. The system prompt is fixed at construction
 * and never changes, enabling prompt caching.
 *
 * Lifecycle:
 *   1. Construct with system prompt and config (once per pipeline run)
 *   2. Call generate() for the initial scenario set
 *   3. After verification, call regenerate(feedback) with repair feedback
 *   4. Session is GC'd when the pipeline finishes (no explicit close needed)
 */
export class ScenarioGeneratorSession {
  private readonly systemPrompt: string | SystemModelMessage;
  private readonly model: LanguageModel;
  private readonly schema: z.ZodType;
  private readonly handwrittenScenarios: TestScenario[];

  /**
   * Accumulated conversation history (user/assistant message pairs).
   * Grows across turns; system prompt is separate and stable.
   */
  private readonly history: Array<{ role: 'user' | 'assistant'; content: string }>;

  constructor(options: {
    system: string | SystemModelMessage;
    model: LanguageModel;
    annotations: ToolAnnotation[];
    handwrittenScenarios: TestScenario[];
  });

  /**
   * Initial generation: sends the first user message and returns scenarios.
   * This is semantically equivalent to the existing single-shot generateScenarios().
   */
  generate(onProgress?: (message: string) => void): Promise<TestScenario[]>;

  /**
   * Follow-up generation: feeds back corrections and requests replacement
   * scenarios for the ones that were wrong.
   *
   * Returns ONLY the new/replacement scenarios (not handwritten, not
   * previously-generated-and-still-valid ones). The caller is responsible
   * for merging these into the full scenario set.
   */
  regenerate(
    feedback: ScenarioFeedback,
    onProgress?: (message: string) => void,
  ): Promise<TestScenario[]>;

  /** Returns the number of turns completed so far. */
  get turnCount(): number;
}
```

### Changes to `generateObjectWithRepair`

No changes. Each turn in the session makes its own call to `generateObjectWithRepair` with a `prompt` that includes the full message history via `generateText`'s `messages` parameter. However, to support this, the session needs to call `generateText` directly (not through `generateObjectWithRepair`) since it manages its own message history.

**Revised approach**: The session calls `generateText` directly with its accumulated `messages` array and the stable `system` prompt. Schema validation and extraction use the same `extractJson` + `schema.parse` pattern from `generateObjectWithRepair`, extracted into a shared helper.

### Extracted utility: `parseJsonWithSchema`

```typescript
// src/pipeline/generate-with-repair.ts

/**
 * Extracts and validates JSON from LLM text output against a Zod schema.
 * Handles markdown fences, surrounding prose, and schema validation.
 *
 * Exported for use by ScenarioGeneratorSession which manages its own
 * message history but needs the same extraction+validation logic.
 */
export function parseJsonWithSchema<T extends z.ZodType>(
  text: string,
  schema: T,
): z.infer<T>;
```

This is a pure extraction of the existing `extractJson` + `JSON.parse` + `schema.parse` logic from the `generateObjectWithRepair` loop body. The function throws on parse failure (same behavior as today).

## Conversation History Structure

### Turn 1: Initial generation

```
System: [buildGeneratorSystemPrompt -- stable, cacheable]
User: "Generate test scenarios following the instructions above.
       [JSON schema hint appended by session]"
Assistant: "{ scenarios: [...] }"
```

### Turn 2+: Regeneration with feedback

```
System: [same system prompt -- cache hit]
User (turn 1): [same as above -- cache hit with history breakpoint]
Assistant (turn 1): [same as above -- cache hit with history breakpoint]
User (turn 2): [feedback message -- new content]

  "Some of your scenarios had incorrect expectations. Here is what happened:

  ## Corrected Scenarios
  These scenarios had wrong expectedDecision values. The verifier determined
  the correct decisions:
  - "Read file in /etc": you said 'deny', correct is 'escalate' (reason: ...)
  - "Move file from sandbox to /tmp": you said 'allow', correct is 'deny' (reason: ...)

  ## Discarded Scenarios (Structural Conflicts)
  These scenarios conflict with hardcoded structural invariants and were
  removed. Do not regenerate them:
  - "Write to audit.jsonl": structural-protected-paths always returns deny
  - "Unknown tool call": structural-unknown-tool always returns deny

  ## Coverage Gaps Found by Verifier
  The verifier generated probe scenarios that found gaps. Consider these
  areas when generating replacement scenarios:
  - filesystem/read_file {path: '/var/log/auth.log'} -> escalate
  - filesystem/write_file {path: '/home/user/Downloads/report.txt'} -> allow

  Generate replacement scenarios for the corrected ones above. Keep your
  new scenarios consistent with the corrections. Do not repeat discarded
  scenarios."

Assistant (turn 2): "{ scenarios: [...] }"
```

### Prompt caching behavior

With Anthropic's prompt caching, the system prompt and all messages up to the penultimate one are cacheable. The session uses `cacheStrategy.applyHistoryBreakpoint()` (already available) to mark the cache boundary before each `generateText` call. On turn 2+, the system prompt and turn-1 messages are served from cache, so only the new feedback message incurs input token costs.

## Integration with Compile-Verify-Repair Loop

### Current flow (simplified from `compile.ts:main()`):

```
1. compilePolicyRules()
2. generateTestScenarios()         // single-shot
3. filterStructuralConflicts()
4. verifyCompiledPolicy()
5. if failures:
   for attempt in 1..MAX_REPAIRS:
     a. extractScenarioCorrections() // blame attribution
     b. applyScenarioCorrections()   // patch expectations
     c. compilePolicyRulesWithRepair() // if rule-blamed
     d. verifyCompiledPolicy()       // re-verify
```

### Proposed flow:

```
1. compilePolicyRules()
2. Create ScenarioGeneratorSession
3. session.generate()               // turn 1
4. filterStructuralConflicts()
5. verifyCompiledPolicy()
6. if failures:
   for attempt in 1..MAX_REPAIRS:
     a. extractScenarioCorrections()
     b. applyScenarioCorrections()   // still applied to main list
     c. session.regenerate(feedback)  // turn 2+: get replacements
     d. merge replacements into scenario list
     e. filterStructuralConflicts()  // re-filter replacements
     f. compilePolicyRulesWithRepair()
     g. verifyCompiledPolicy()
```

The key change: step (c) feeds corrections + discarded + probes into the session's `regenerate()` method, which returns replacement scenarios. These replacements are merged into (not appended to) the scenario list, substituting for the corrected/discarded ones.

### What "merge" means concretely

```typescript
// In compile.ts repair loop:
const replacements = await session.regenerate({
  corrections,
  discardedScenarios,
  probeScenarios: accumulatedProbes,
});

// Replace corrected scenarios with their regenerated versions.
// Keep all scenarios that were NOT corrected or discarded.
const correctedDescriptions = new Set(corrections.map(c => c.scenarioDescription));
const discardedDescriptions = new Set(
  discardedScenarios.map(d => d.scenario.description)
);
const kept = scenarioResult.scenarios.filter(
  s => !correctedDescriptions.has(s.description) &&
       !discardedDescriptions.has(s.description)
);

// Deduplicate replacements against kept and handwritten scenarios
const keptDescriptions = new Set(kept.map(s => s.description));
const uniqueReplacements = replacements.filter(
  r => !keptDescriptions.has(r.description)
);

scenarioResult.scenarios = [...kept, ...uniqueReplacements];
```

## Files Changed

### `src/pipeline/types.ts`

Add `ScenarioFeedback` interface (shown above). One new interface, ~15 lines.

### `src/pipeline/generate-with-repair.ts`

Export `parseJsonWithSchema()` -- extract the existing `extractJson` + `JSON.parse` + `schema.parse` logic into a named function. Also export `extractJson` (currently file-private) since the session needs it. The `generateObjectWithRepair` function body calls the new helper instead of inlining the logic. Net change: ~10 lines, pure refactor.

```typescript
// Before (inside generateObjectWithRepair loop):
const json: unknown = JSON.parse(extractJson(text));
const parsed = schema.parse(json) as z.infer<T>;

// After:
const parsed = parseJsonWithSchema(text, schema);
```

### `src/pipeline/scenario-generator.ts`

Add `ScenarioGeneratorSession` class (~80 lines). The existing `generateScenarios()` function and `buildGeneratorSystemPrompt()` remain unchanged -- the session uses them internally for the first turn and delegates prompt construction to the existing function.

Key implementation details:

```typescript
export class ScenarioGeneratorSession {
  private readonly systemPrompt: string | SystemModelMessage;
  private readonly model: LanguageModel;
  private readonly schema: ReturnType<typeof buildGeneratorResponseSchema>;
  private readonly handwrittenScenarios: TestScenario[];
  private readonly history: Array<{ role: 'user' | 'assistant'; content: string }> = [];
  private readonly schemaHint: string;
  private turns = 0;

  constructor(options: {
    system: string | SystemModelMessage;
    model: LanguageModel;
    annotations: ToolAnnotation[];
    handwrittenScenarios: TestScenario[];
  }) {
    this.systemPrompt = options.system;
    this.model = options.model;
    this.handwrittenScenarios = options.handwrittenScenarios;

    const serverNames = [...new Set(options.annotations.map(a => a.serverName))] as [string, ...string[]];
    const toolNames = [...new Set(options.annotations.map(a => a.toolName))] as [string, ...string[]];
    this.schema = buildGeneratorResponseSchema(serverNames, toolNames);
    this.schemaHint = schemaToPromptHint(this.schema);
  }

  async generate(onProgress?: (message: string) => void): Promise<TestScenario[]> {
    const userMessage = 'Generate test scenarios following the instructions above.' + this.schemaHint;
    this.history.push({ role: 'user', content: userMessage });

    const result = await generateText({
      model: this.model,
      system: this.systemPrompt,
      messages: this.history,
      maxOutputTokens: 8192,
    });

    this.history.push({ role: 'assistant', content: result.text });
    this.turns++;

    const output = parseJsonWithSchema(result.text, this.schema);
    const generated: TestScenario[] = output.scenarios.map((s: TestScenario) => ({
      ...s,
      source: 'generated' as const,
    }));

    // Deduplicate against handwritten
    const unique = generated.filter(
      g => !this.handwrittenScenarios.some(h => areSimilar(g, h))
    );
    return [...this.handwrittenScenarios, ...unique];
  }

  async regenerate(
    feedback: ScenarioFeedback,
    onProgress?: (message: string) => void,
  ): Promise<TestScenario[]> {
    const feedbackMessage = formatFeedbackMessage(feedback) + this.schemaHint;
    this.history.push({ role: 'user', content: feedbackMessage });

    onProgress?.(`Regenerating scenarios (turn ${this.turns + 1})...`);

    const result = await generateText({
      model: this.model,
      system: this.systemPrompt,
      messages: this.history,
      maxOutputTokens: 8192,
    });

    this.history.push({ role: 'assistant', content: result.text });
    this.turns++;

    const output = parseJsonWithSchema(result.text, this.schema);
    return output.scenarios.map((s: TestScenario) => ({
      ...s,
      source: 'generated' as const,
    }));
  }

  get turnCount(): number {
    return this.turns;
  }
}
```

The `formatFeedbackMessage()` helper (file-private) constructs the feedback user message from `ScenarioFeedback`:

```typescript
function formatFeedbackMessage(feedback: ScenarioFeedback): string {
  const sections: string[] = [];

  if (feedback.corrections.length > 0) {
    const lines = feedback.corrections.map(
      c => `- "${c.scenarioDescription}": correct decision is ${c.correctedDecision} (${c.correctedReasoning})`
    );
    sections.push(
      '## Corrected Scenarios\n\n' +
      'These scenarios had wrong expectedDecision values. The verifier determined the correct decisions:\n\n' +
      lines.join('\n')
    );
  }

  if (feedback.discardedScenarios.length > 0) {
    const lines = feedback.discardedScenarios.map(
      d => `- "${d.scenario.description}": ${d.rule} always returns ${d.actual}`
    );
    sections.push(
      '## Discarded Scenarios (Structural Conflicts)\n\n' +
      'These scenarios conflict with hardcoded structural invariants and were removed. Do NOT regenerate them:\n\n' +
      lines.join('\n')
    );
  }

  if (feedback.probeScenarios.length > 0) {
    const lines = feedback.probeScenarios.map(
      p => `- ${p.request.serverName}/${p.request.toolName} ${JSON.stringify(p.request.arguments)} -> ${p.expectedDecision}`
    );
    sections.push(
      '## Coverage Gaps Found by Verifier\n\n' +
      'The verifier generated probe scenarios that found gaps. Consider these areas:\n\n' +
      lines.join('\n')
    );
  }

  return sections.join('\n\n') +
    '\n\nGenerate replacement scenarios for the corrected and discarded ones above. ' +
    'Keep your new scenarios consistent with the corrections. ' +
    'Do not repeat discarded scenarios or reproduce the original wrong expectations.\n';
}
```

### `src/pipeline/compile.ts`

Changes to the `main()` function:

1. **Create session instead of calling `generateTestScenarios()` directly.** Replace the `generateTestScenarios()` call with session creation + `session.generate()`. The `generateTestScenarios()` wrapper function can remain for backward compatibility but internally delegates to the session.

2. **In the repair loop, call `session.regenerate()`.** After `applyScenarioCorrections()`, build a `ScenarioFeedback` and call `session.regenerate()`. Merge the returned replacements into the scenario list.

3. **Pass accumulated discarded scenarios into the feedback.** The existing `discardedScenarios` array (from `filterAndLogStructuralConflicts`) is already tracked -- it just needs to be passed through.

Here is the delta in pseudocode (actual lines changed in `main()`):

```typescript
// Before:
const scenarioResult = await generateTestScenarios(...);

// After:
let session: ScenarioGeneratorSession | undefined;
const scenarioResult = await generateTestScenarios(
  ...,
  // NEW: receive back the session for later regeneration
  (s) => { session = s; },
);

// ... later, in the repair loop:

// Before (existing code):
if (corrections.length > 0) {
  scenarioResult.scenarios = applyScenarioCorrections(scenarioResult.scenarios, corrections);
  // ...
}

// After (added after applyScenarioCorrections):
if (session && (corrections.length > 0 || discardedScenarios.length > 0)) {
  const replacements = await session.regenerate({
    corrections,
    discardedScenarios,
    probeScenarios: accumulatedProbes,
  });

  // Remove corrected/discarded, merge in replacements
  const removedDescriptions = new Set([
    ...corrections.map(c => c.scenarioDescription),
    ...discardedScenarios.filter(d => d.scenario.source !== 'handwritten')
      .map(d => d.scenario.description),
  ]);
  const kept = scenarioResult.scenarios.filter(
    s => !removedDescriptions.has(s.description)
  );
  const keptDescriptions = new Set(kept.map(s => s.description));
  const uniqueReplacements = replacements.filter(
    r => !keptDescriptions.has(r.description)
  );
  scenarioResult.scenarios = [...kept, ...uniqueReplacements];
}
```

Estimated delta: ~30 lines changed in `compile.ts`.

## Component Diagram

```
compile.ts (orchestrator)
  |
  |-- creates --> ScenarioGeneratorSession
  |                 |
  |                 |-- owns --> message history (Array<{role, content}>)
  |                 |-- holds --> system prompt (stable, cacheable)
  |                 |-- calls --> generateText() from AI SDK
  |                 |-- calls --> parseJsonWithSchema() from generate-with-repair.ts
  |                 |
  |                 |-- generate()     --> turn 1 (initial scenarios)
  |                 |-- regenerate(fb) --> turn 2+ (replacement scenarios)
  |                 |
  |-- calls --> filterStructuralConflicts() --> DiscardedScenario[]
  |-- calls --> verifyCompiledPolicy()      --> VerificationResult
  |-- calls --> extractScenarioCorrections() --> ScenarioCorrection[]
  |
  |-- assembles ScenarioFeedback from:
  |     corrections + discardedScenarios + accumulatedProbes
  |-- passes to session.regenerate()
  |-- merges replacements into scenarioResult.scenarios
```

## Extension Points

1. **Additional feedback types.** `ScenarioFeedback` can grow new optional fields (e.g., `ruleChanges` to tell the generator about recompiled rules) without breaking existing callers. Since all fields are optional and readonly, this is backward-compatible.

2. **Configurable turn limit.** The session could accept a `maxTurns` option and refuse to regenerate beyond it, providing a safety valve against runaway loops.

3. **History serialization.** If in the future we want to persist the conversation for debugging, the `history` array is plain data and trivially serializable. The session could expose a `getHistory()` method for diagnostic logging.

4. **Use by other pipeline stages.** The same pattern (stateful session + stable system prompt + growing message history) could be applied to the constitution compiler if it also needs multi-turn repair awareness. The `generateObjectWithRepair` utility would remain the lower-level primitive for schema repair within a single turn.

## Testing Strategy

### Unit tests for `ScenarioGeneratorSession`

The session depends on `LanguageModel` (an interface from AI SDK) and `parseJsonWithSchema` (a pure function). Tests can:

1. **Mock the language model** to return predetermined JSON responses. Verify that:
   - Turn 1 sends the correct initial prompt with schema hint
   - Turn 2 sends the feedback message with corrections formatted correctly
   - The message history grows correctly across turns
   - Handwritten scenarios are preserved and not deduplicated away

2. **Test `formatFeedbackMessage`** as a pure function:
   - Empty feedback produces minimal message
   - Corrections are formatted with description, decision, and reasoning
   - Discarded scenarios include the structural rule name
   - Probe scenarios include tool name and arguments

3. **Test the merge logic** in `compile.ts` (can be extracted into a helper):
   - Corrected scenarios are replaced by regenerated ones
   - Uncorrected scenarios are kept
   - Duplicates (by description) are removed
   - Handwritten scenarios are never removed

### Unit tests for `parseJsonWithSchema`

- Valid JSON in markdown fences
- Valid JSON with surrounding prose
- Invalid JSON throws
- Valid JSON that fails schema validation throws with Zod error

### Integration test

A focused test in `test/scenario-generator.test.ts`:
- Create a session with a mock LLM
- Call `generate()`, verify initial scenarios
- Call `regenerate()` with corrections, verify the LLM received the feedback in its messages
- Verify the system prompt is identical across both calls (important for cache behavior)

## Migration Notes

### Backward Compatibility

The existing `generateScenarios()` function is preserved as-is. It remains the simple single-shot API for callers that don't need multi-turn behavior (e.g., tests, future tooling). The `ScenarioGeneratorSession` is an additive layer on top.

### Incremental Rollout

The change can be shipped in a single PR:

1. Extract `parseJsonWithSchema` and `schemaToPromptHint` from `generate-with-repair.ts` (pure refactor, no behavior change)
2. Add `ScenarioFeedback` to `types.ts`
3. Add `ScenarioGeneratorSession` to `scenario-generator.ts`
4. Update `compile.ts` to use the session in the repair loop

The `generateTestScenarios()` wrapper in `compile.ts` can be updated to optionally return the session via a callback or by returning a richer result type:

```typescript
interface ScenarioGenerationResult {
  scenarios: TestScenario[];
  inputHash: string;
  session?: ScenarioGeneratorSession;  // undefined when cached
}
```

This avoids changing the function signature in a breaking way -- the `session` field is simply `undefined` when the cache hit path was taken (no LLM call, no session created).

### What NOT to Change

- `generateObjectWithRepair` -- it stays focused on schema-repair for a single logical request. Multi-turn is the session's job.
- `buildGeneratorSystemPrompt` -- it stays a pure function that builds the prompt string. The session calls it at construction time.
- Handwritten scenarios -- they are never regenerated or modified by the session. They pass through unchanged.
- The content-hash caching in `generateTestScenarios` -- the session is only created on cache miss. If the hash matches, the existing artifact is used and no session is needed.
